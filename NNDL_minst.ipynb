{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.1+cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchsummary import summary \n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=512 #大概需要2G的显存\n",
    "EPOCHS=20 # 总共训练批次\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_loader的功能就是把数据加载进来，设置成想要的格式然后按batchsize 分割好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data', train=True, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),#把灰度映射到0到1\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))#对图像数据进行标准化 后面两个参数分别是各个通道的均值和标准差\n",
    "                       ])),\n",
    "        batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个data里有512张图片，整个train_loader里一共有60000张图片 分成118块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer= SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_batch,label_batch=next(iter(train_loader))\n",
    "img_grid=torchvision.utils.make_grid(data_batch,nrow=32,normalize=True,scale_each=True)\n",
    "writer.add_image('input_image_2',img_grid,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset 里有固定的测试集合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#structure 1\n",
    "class ConvNet1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # batch*1*28*28（每次会送入batch个样本，输入通道数1（黑白图像），图像分辨率是28x28）\n",
    "        # 下面的卷积层Conv2d的第一个参数指输入通道数，第二个参数指输出通道数，第三个参数指卷积核的大小\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5) # 输入通道数1，输出通道数10，核的大小5\n",
    "        self.conv2 = nn.Conv2d(10, 20, 3) # 输入通道数10，输出通道数20，核的大小3\n",
    "        # 下面的全连接层Linear的第一个参数指输入通道数，第二个参数指输出通道数\n",
    "        self.fc1 = nn.Linear(20*10*10, 50) # 输入通道数是2000，输出通道数是500\n",
    "        self.fc2 = nn.Linear(50, 10) # 输入通道数是500，输出通道数是10，即10分类\n",
    "    def forward(self,x):\n",
    "        in_size = x.size(0) # 在本例中in_size=512，也就是BATCH_SIZE的值。输入的x可以看成是512*1*28*28的张量。\n",
    "        out = self.conv1(x) # batch*1*28*28 -> batch*10*24*24（28x28的图像经过一次核为5x5的卷积，输出变为24x24）\n",
    "        out = F.relu(out) # batch*10*24*24（激活函数ReLU不改变形状））\n",
    "        out = F.max_pool2d(out, 2, 2) # batch*10*24*24 -> batch*10*12*12（2*2的池化层会减半）\n",
    "        out = self.conv2(out) # batch*10*12*12 -> batch*20*10*10（再卷积一次，核的大小是3）\n",
    "        out = F.relu(out) # batch*20*10*10\n",
    "        out = out.view(in_size, -1) # batch*20*10*10 -> batch*2000（out的第二维是-1，说明是自动推算，本例中第二维是20*10*10）\n",
    "        out = self.fc1(out) # batch*2000 -> batch*500\n",
    "        out = F.relu(out) # batch*500\n",
    "        out = self.fc2(out) # batch*500 -> batch*10\n",
    "        out = F.log_softmax(out, dim=1) # 计算log(softmax(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#structure 2\n",
    "class ConvNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # batch*1*28*28（每次会送入batch个样本，输入通道数1（黑白图像），图像分辨率是28x28）\n",
    "        # 下面的卷积层Conv2d的第一个参数指输入通道数，第二个参数指输出通道数，第三个参数指卷积核的大小\n",
    "        self.conv1 = nn.Conv2d(1, 1, 3) # 输入通道数1，输出通道数10，核的大小5\n",
    "        self.conv3 = nn.Conv2d(1, 10, 3)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 3) # 输入通道数10，输出通道数20，核的大小3\n",
    "        # 下面的全连接层Linear的第一个参数指输入通道数，第二个参数指输出通道数\n",
    "        self.fc1 = nn.Linear(20*10*10, 50) # 输入通道数是2000，输出通道数是500\n",
    "        self.fc2 = nn.Linear(50, 10) # 输入通道数是500，输出通道数是10，即10分类\n",
    "    def forward(self,x):\n",
    "        in_size = x.size(0) # 在本例中in_size=512，也就是BATCH_SIZE的值。输入的x可以看成是512*1*28*28的张量。\n",
    "        out = self.conv1(x) # batch*1*28*28 -> batch*10*24*24（28x28的图像经过一次核为5x5的卷积，输出变为24x24）\n",
    "        out=self.conv3(out)\n",
    "        out = F.relu(out) # batch*10*24*24（激活函数ReLU不改变形状））\n",
    "        out = F.max_pool2d(out, 2, 2) # batch*10*24*24 -> batch*10*12*12（2*2的池化层会减半）\n",
    "        out = self.conv2(out) # batch*10*12*12 -> batch*20*10*10（再卷积一次，核的大小是3）\n",
    "        out = F.relu(out) # batch*20*10*10\n",
    "        out = out.view(in_size, -1) # batch*20*10*10 -> batch*2000（out的第二维是-1，说明是自动推算，本例中第二维是20*10*10）\n",
    "        out = self.fc1(out) # batch*2000 -> batch*500\n",
    "        out = F.relu(out) # batch*500\n",
    "        out = self.fc2(out) # batch*500 -> batch*10\n",
    "        out = F.log_softmax(out, dim=1) # 计算log(softmax(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure 3\n",
    "class ConvNet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # batch*1*28*28（每次会送入batch个样本，输入通道数1（黑白图像），图像分辨率是28x28）\n",
    "        # 下面的卷积层Conv2d的第一个参数指输入通道数，第二个参数指输出通道数，第三个参数指卷积核的大小\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5) # 输入通道数1，输出通道数10，核的大小5\n",
    "        self.conv2 = nn.Conv2d(10, 20, 3) # 输入通道数10，输出通道数20，核的大小3\n",
    "        # 下面的全连接层Linear的第一个参数指输入通道数，第二个参数指输出通道数\n",
    "        self.fc1 = nn.Linear(20*10*10, 50) # 输入通道数是2000，输出通道数是500\n",
    "        self.fc2 = nn.Linear(50, 10) # 输入通道数是500，输出通道数是10，即10分类\n",
    "    def forward(self,x):\n",
    "        in_size = x.size(0) # 在本例中in_size=512，也就是BATCH_SIZE的值。输入的x可以看成是512*1*28*28的张量。\n",
    "        out = self.conv1(x) # batch*1*28*28 -> batch*10*24*24（28x28的图像经过一次核为5x5的卷积，输出变为24x24）\n",
    "        out = torch.sigmoid(out) # batch*10*24*24（激活函数ReLU不改变形状））\n",
    "        out = F.max_pool2d(out, 2, 2) # batch*10*24*24 -> batch*10*12*12（2*2的池化层会减半）\n",
    "        out = self.conv2(out) # batch*10*12*12 -> batch*20*10*10（再卷积一次，核的大小是3）\n",
    "        out = torch.sigmoid(out) # batch*20*10*10\n",
    "        out = out.view(in_size, -1) # batch*20*10*10 -> batch*2000（out的第二维是-1，说明是自动推算，本例中第二维是20*10*10）\n",
    "        out = self.fc1(out) # batch*2000 -> batch*500\n",
    "        out = torch.sigmoid(out) # batch*500\n",
    "        out = self.fc2(out) # batch*500 -> batch*10\n",
    "        out = F.log_softmax(out, dim=1) # 计算log(softmax(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造训练测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    accuracy=0\n",
    "    train_loss=0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        #这行代码的意思是将所有最开始读取数据时的tensor变量copy一份到device所指定的GPU上去，之后的运算都在GPU上进行\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        loss = F.nll_loss(output, target)\n",
    "        train_loss+=loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(batch_idx+1)%30 == 0: #118组数据每次取30相当于1/4\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    train_loss/=len(train_loader.dataset)\n",
    "    accuracy=correct/len(train_loader.dataset)\n",
    "    writer.add_scalar('loss/epoch',train_loss.item(), epoch)\n",
    "    #writer.add_scalar('train/epoch',accuracy, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    accuracy=0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # 将一批的损失相加\n",
    "            pred = output.max(1, keepdim=True)[1] # 找到概率最大的下标\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    accuracy=100. * correct / len(test_loader.dataset)\n",
    "    test_loss /= len(test_loader.dataset)#输出的是平均损失\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    writer.add_scalar('test_loss/epoch',test_loss,epoch)\n",
    "    writer.add_scalar('test/epoch',accuracy, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回一个epoch内iteration的图像\n",
    "def train_withinepoch(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    accuracy=0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        correct = 0\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        #这行代码的意思是将所有最开始读取数据时的tensor变量copy一份到device所指定的GPU上去，之后的运算都在GPU上进行\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        accuracy=correct/len(data)   \n",
    "        writer.add_scalar('act_train_loss/iteration',loss.item(), batch_idx)\n",
    "        writer.add_scalar('act_train_accu/iteration',accuracy, batch_idx)\n",
    "        if(batch_idx+1)%30 == 0: #118组数据每次取30相当于1/4\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = ConvNet1().to(DEVICE)\n",
    "optimizer1 = optim.Adam(model1.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ConvNet2().to(DEVICE)\n",
    "optimizer2 = optim.Adam(model2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = ConvNet3().to(DEVICE)\n",
    "optimizer3 = optim.Adam(model3.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 10, 24, 24]             260\n",
      "            Conv2d-2           [-1, 20, 10, 10]           1,820\n",
      "            Linear-3                   [-1, 50]         100,050\n",
      "            Linear-4                   [-1, 10]             510\n",
      "================================================================\n",
      "Total params: 102,640\n",
      "Trainable params: 102,640\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.06\n",
      "Params size (MB): 0.39\n",
      "Estimated Total Size (MB): 0.45\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(summary(model1,(1,28,28),device='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 1, 26, 26]              10\n",
      "            Conv2d-2           [-1, 10, 24, 24]             100\n",
      "            Conv2d-3           [-1, 20, 10, 10]           1,820\n",
      "            Linear-4                   [-1, 50]         100,050\n",
      "            Linear-5                   [-1, 10]             510\n",
      "================================================================\n",
      "Total params: 102,490\n",
      "Trainable params: 102,490\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.06\n",
      "Params size (MB): 0.39\n",
      "Estimated Total Size (MB): 0.46\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(summary(model2,(1,28,28),device='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 10, 24, 24]             260\n",
      "            Conv2d-2           [-1, 20, 10, 10]           1,820\n",
      "            Linear-3                   [-1, 50]         100,050\n",
      "            Linear-4                   [-1, 10]             510\n",
      "================================================================\n",
      "Total params: 102,640\n",
      "Trainable params: 102,640\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.06\n",
      "Params size (MB): 0.39\n",
      "Estimated Total Size (MB): 0.45\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(summary(model3,(1,28,28),device='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "structure 1 的表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [14848/60000 (25%)]\tLoss: 0.430253\n",
      "Train Epoch: 1 [30208/60000 (50%)]\tLoss: 0.315362\n",
      "Train Epoch: 1 [45568/60000 (75%)]\tLoss: 0.277270\n",
      "\n",
      "Test set: Average loss: 0.1690, Accuracy: 9518/10000 (95%)\n",
      "\n",
      "Train Epoch: 2 [14848/60000 (25%)]\tLoss: 0.102387\n",
      "Train Epoch: 2 [30208/60000 (50%)]\tLoss: 0.089983\n",
      "Train Epoch: 2 [45568/60000 (75%)]\tLoss: 0.107243\n",
      "\n",
      "Test set: Average loss: 0.0882, Accuracy: 9747/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [14848/60000 (25%)]\tLoss: 0.099168\n",
      "Train Epoch: 3 [30208/60000 (50%)]\tLoss: 0.090412\n",
      "Train Epoch: 3 [45568/60000 (75%)]\tLoss: 0.055350\n",
      "\n",
      "Test set: Average loss: 0.0604, Accuracy: 9793/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [14848/60000 (25%)]\tLoss: 0.068430\n",
      "Train Epoch: 4 [30208/60000 (50%)]\tLoss: 0.067210\n",
      "Train Epoch: 4 [45568/60000 (75%)]\tLoss: 0.065809\n",
      "\n",
      "Test set: Average loss: 0.0579, Accuracy: 9808/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [14848/60000 (25%)]\tLoss: 0.040630\n",
      "Train Epoch: 5 [30208/60000 (50%)]\tLoss: 0.039512\n",
      "Train Epoch: 5 [45568/60000 (75%)]\tLoss: 0.079267\n",
      "\n",
      "Test set: Average loss: 0.0469, Accuracy: 9852/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [14848/60000 (25%)]\tLoss: 0.044243\n",
      "Train Epoch: 6 [30208/60000 (50%)]\tLoss: 0.023823\n",
      "Train Epoch: 6 [45568/60000 (75%)]\tLoss: 0.050273\n",
      "\n",
      "Test set: Average loss: 0.0497, Accuracy: 9841/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [14848/60000 (25%)]\tLoss: 0.048661\n",
      "Train Epoch: 7 [30208/60000 (50%)]\tLoss: 0.054507\n",
      "Train Epoch: 7 [45568/60000 (75%)]\tLoss: 0.042212\n",
      "\n",
      "Test set: Average loss: 0.0401, Accuracy: 9864/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [14848/60000 (25%)]\tLoss: 0.025108\n",
      "Train Epoch: 8 [30208/60000 (50%)]\tLoss: 0.035279\n",
      "Train Epoch: 8 [45568/60000 (75%)]\tLoss: 0.028328\n",
      "\n",
      "Test set: Average loss: 0.0387, Accuracy: 9871/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [14848/60000 (25%)]\tLoss: 0.021408\n",
      "Train Epoch: 9 [30208/60000 (50%)]\tLoss: 0.040559\n",
      "Train Epoch: 9 [45568/60000 (75%)]\tLoss: 0.055947\n",
      "\n",
      "Test set: Average loss: 0.0399, Accuracy: 9876/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [14848/60000 (25%)]\tLoss: 0.037175\n",
      "Train Epoch: 10 [30208/60000 (50%)]\tLoss: 0.011330\n",
      "Train Epoch: 10 [45568/60000 (75%)]\tLoss: 0.018685\n",
      "\n",
      "Test set: Average loss: 0.0386, Accuracy: 9882/10000 (99%)\n",
      "\n",
      "Train Epoch: 11 [14848/60000 (25%)]\tLoss: 0.020899\n",
      "Train Epoch: 11 [30208/60000 (50%)]\tLoss: 0.026674\n",
      "Train Epoch: 11 [45568/60000 (75%)]\tLoss: 0.025343\n",
      "\n",
      "Test set: Average loss: 0.0353, Accuracy: 9890/10000 (99%)\n",
      "\n",
      "Train Epoch: 12 [14848/60000 (25%)]\tLoss: 0.021248\n",
      "Train Epoch: 12 [30208/60000 (50%)]\tLoss: 0.026056\n",
      "Train Epoch: 12 [45568/60000 (75%)]\tLoss: 0.061614\n",
      "\n",
      "Test set: Average loss: 0.0354, Accuracy: 9890/10000 (99%)\n",
      "\n",
      "Train Epoch: 13 [14848/60000 (25%)]\tLoss: 0.018726\n",
      "Train Epoch: 13 [30208/60000 (50%)]\tLoss: 0.014942\n",
      "Train Epoch: 13 [45568/60000 (75%)]\tLoss: 0.014342\n",
      "\n",
      "Test set: Average loss: 0.0364, Accuracy: 9877/10000 (99%)\n",
      "\n",
      "Train Epoch: 14 [14848/60000 (25%)]\tLoss: 0.012666\n",
      "Train Epoch: 14 [30208/60000 (50%)]\tLoss: 0.012890\n",
      "Train Epoch: 14 [45568/60000 (75%)]\tLoss: 0.012695\n",
      "\n",
      "Test set: Average loss: 0.0337, Accuracy: 9894/10000 (99%)\n",
      "\n",
      "Train Epoch: 15 [14848/60000 (25%)]\tLoss: 0.012927\n",
      "Train Epoch: 15 [30208/60000 (50%)]\tLoss: 0.013221\n",
      "Train Epoch: 15 [45568/60000 (75%)]\tLoss: 0.015950\n",
      "\n",
      "Test set: Average loss: 0.0331, Accuracy: 9894/10000 (99%)\n",
      "\n",
      "Train Epoch: 16 [14848/60000 (25%)]\tLoss: 0.012735\n",
      "Train Epoch: 16 [30208/60000 (50%)]\tLoss: 0.013146\n",
      "Train Epoch: 16 [45568/60000 (75%)]\tLoss: 0.007570\n",
      "\n",
      "Test set: Average loss: 0.0344, Accuracy: 9891/10000 (99%)\n",
      "\n",
      "Train Epoch: 17 [14848/60000 (25%)]\tLoss: 0.010972\n",
      "Train Epoch: 17 [30208/60000 (50%)]\tLoss: 0.015982\n",
      "Train Epoch: 17 [45568/60000 (75%)]\tLoss: 0.006547\n",
      "\n",
      "Test set: Average loss: 0.0421, Accuracy: 9865/10000 (99%)\n",
      "\n",
      "Train Epoch: 18 [14848/60000 (25%)]\tLoss: 0.004267\n",
      "Train Epoch: 18 [30208/60000 (50%)]\tLoss: 0.012668\n",
      "Train Epoch: 18 [45568/60000 (75%)]\tLoss: 0.007103\n",
      "\n",
      "Test set: Average loss: 0.0350, Accuracy: 9892/10000 (99%)\n",
      "\n",
      "Train Epoch: 19 [14848/60000 (25%)]\tLoss: 0.003844\n",
      "Train Epoch: 19 [30208/60000 (50%)]\tLoss: 0.013767\n",
      "Train Epoch: 19 [45568/60000 (75%)]\tLoss: 0.011369\n",
      "\n",
      "Test set: Average loss: 0.0428, Accuracy: 9870/10000 (99%)\n",
      "\n",
      "Train Epoch: 20 [14848/60000 (25%)]\tLoss: 0.013470\n",
      "Train Epoch: 20 [30208/60000 (50%)]\tLoss: 0.002533\n",
      "Train Epoch: 20 [45568/60000 (75%)]\tLoss: 0.006547\n",
      "\n",
      "Test set: Average loss: 0.0372, Accuracy: 9894/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with SummaryWriter('D:\\\\Document\\\\python_text\\\\runs'+'\\\\Linear') as writer:\n",
    "    for epoch in range(1, EPOCHS + 1):#可以看出一个epoch就是数据集上的所有样本都跑完一遍\n",
    "        train(model1, DEVICE, train_loader, optimizer1, epoch)\n",
    "        \n",
    "        test(model1, DEVICE, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [14848/60000 (25%)]\tLoss: 2.289826\n",
      "Train Epoch: 1 [30208/60000 (50%)]\tLoss: 2.077784\n",
      "Train Epoch: 1 [45568/60000 (75%)]\tLoss: 1.407144\n",
      "Train Epoch: 2 [14848/60000 (25%)]\tLoss: 0.762222\n",
      "Train Epoch: 2 [30208/60000 (50%)]\tLoss: 0.602912\n",
      "Train Epoch: 2 [45568/60000 (75%)]\tLoss: 0.539983\n",
      "Train Epoch: 3 [14848/60000 (25%)]\tLoss: 0.458538\n",
      "Train Epoch: 3 [30208/60000 (50%)]\tLoss: 0.388022\n",
      "Train Epoch: 3 [45568/60000 (75%)]\tLoss: 0.401562\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:75] data. DefaultCPUAllocator: not enough memory: you tried to allocate 12845056 bytes. Buy new RAM!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-192-164a3d20cc42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m#可以看出一个epoch就是数据集上的所有样本都跑完一遍\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-73-6e70a58934dc>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m30\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#118组数据每次取30相当于1/4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:75] data. DefaultCPUAllocator: not enough memory: you tried to allocate 12845056 bytes. Buy new RAM!"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):#可以看出一个epoch就是数据集上的所有样本都跑完一遍\n",
    "        train(model3, DEVICE, train_loader, optimizer3, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较relu函数和sigmoid函数的差别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [14848/60000 (25%)]\tLoss: 0.464572\n",
      "Train Epoch: 1 [30208/60000 (50%)]\tLoss: 0.296220\n",
      "Train Epoch: 1 [45568/60000 (75%)]\tLoss: 0.237985\n"
     ]
    }
   ],
   "source": [
    "with SummaryWriter(conmment='activation_diff') as writer:\n",
    "        train_withinepoch(model1, DEVICE, train_loader, optimizer1, 1)\n",
    "        train_withinepoch(model3, DEVICE, train_loader, optimizer, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跑完之后需要把数据删掉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较structure 1和structure 2之间的区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = ConvNet1().to(DEVICE)\n",
    "optimizer1 = optim.Adam(model1.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在第一个epoch内随着iteration的表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [14848/60000 (25%)]\tLoss: 0.380029\n",
      "Train Epoch: 1 [30208/60000 (50%)]\tLoss: 0.293711\n",
      "Train Epoch: 1 [45568/60000 (75%)]\tLoss: 0.187705\n"
     ]
    }
   ],
   "source": [
    "with SummaryWriter(comment='struc_diff') as writer:\n",
    "        train_withinepoch(model1, DEVICE, train_loader, optimizer1, 1)\n",
    "        train_withinepoch(model2, DEVICE, train_loader, optimizer9, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ConvNet2().to(DEVICE)\n",
    "optimizer2 = optim.Adam(model2.parameters())\n",
    "model1 = ConvNet1().to(DEVICE)\n",
    "optimizer1 = optim.Adam(model1.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在20个epoch内的表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [14848/60000 (25%)]\tLoss: 0.004774\n",
      "Train Epoch: 1 [30208/60000 (50%)]\tLoss: 0.002575\n",
      "Train Epoch: 1 [45568/60000 (75%)]\tLoss: 0.008070\n",
      "\n",
      "Test set: Average loss: 0.0350, Accuracy: 9896/10000 (99%)\n",
      "\n",
      "Train Epoch: 2 [14848/60000 (25%)]\tLoss: 0.004603\n",
      "Train Epoch: 2 [30208/60000 (50%)]\tLoss: 0.009482\n",
      "Train Epoch: 2 [45568/60000 (75%)]\tLoss: 0.008202\n",
      "\n",
      "Test set: Average loss: 0.0353, Accuracy: 9894/10000 (99%)\n",
      "\n",
      "Train Epoch: 3 [14848/60000 (25%)]\tLoss: 0.006560\n",
      "Train Epoch: 3 [30208/60000 (50%)]\tLoss: 0.001629\n",
      "Train Epoch: 3 [45568/60000 (75%)]\tLoss: 0.001703\n",
      "\n",
      "Test set: Average loss: 0.0310, Accuracy: 9903/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [14848/60000 (25%)]\tLoss: 0.003929\n",
      "Train Epoch: 4 [30208/60000 (50%)]\tLoss: 0.010865\n",
      "Train Epoch: 4 [45568/60000 (75%)]\tLoss: 0.001647\n",
      "\n",
      "Test set: Average loss: 0.0346, Accuracy: 9901/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [14848/60000 (25%)]\tLoss: 0.006202\n",
      "Train Epoch: 5 [30208/60000 (50%)]\tLoss: 0.002303\n",
      "Train Epoch: 5 [45568/60000 (75%)]\tLoss: 0.001865\n",
      "\n",
      "Test set: Average loss: 0.0362, Accuracy: 9903/10000 (99%)\n",
      "\n",
      "Train Epoch: 6 [14848/60000 (25%)]\tLoss: 0.005401\n",
      "Train Epoch: 6 [30208/60000 (50%)]\tLoss: 0.002679\n",
      "Train Epoch: 6 [45568/60000 (75%)]\tLoss: 0.004784\n",
      "\n",
      "Test set: Average loss: 0.0444, Accuracy: 9887/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [14848/60000 (25%)]\tLoss: 0.003428\n",
      "Train Epoch: 7 [30208/60000 (50%)]\tLoss: 0.017231\n",
      "Train Epoch: 7 [45568/60000 (75%)]\tLoss: 0.009922\n",
      "\n",
      "Test set: Average loss: 0.0380, Accuracy: 9895/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [14848/60000 (25%)]\tLoss: 0.003276\n",
      "Train Epoch: 8 [30208/60000 (50%)]\tLoss: 0.002718\n",
      "Train Epoch: 8 [45568/60000 (75%)]\tLoss: 0.002409\n",
      "\n",
      "Test set: Average loss: 0.0378, Accuracy: 9892/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [14848/60000 (25%)]\tLoss: 0.001292\n",
      "Train Epoch: 9 [30208/60000 (50%)]\tLoss: 0.003371\n",
      "Train Epoch: 9 [45568/60000 (75%)]\tLoss: 0.002541\n",
      "\n",
      "Test set: Average loss: 0.0368, Accuracy: 9900/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [14848/60000 (25%)]\tLoss: 0.001797\n",
      "Train Epoch: 10 [30208/60000 (50%)]\tLoss: 0.002892\n",
      "Train Epoch: 10 [45568/60000 (75%)]\tLoss: 0.008857\n",
      "\n",
      "Test set: Average loss: 0.0390, Accuracy: 9897/10000 (99%)\n",
      "\n",
      "Train Epoch: 11 [14848/60000 (25%)]\tLoss: 0.001488\n",
      "Train Epoch: 11 [30208/60000 (50%)]\tLoss: 0.000484\n",
      "Train Epoch: 11 [45568/60000 (75%)]\tLoss: 0.010504\n",
      "\n",
      "Test set: Average loss: 0.0442, Accuracy: 9892/10000 (99%)\n",
      "\n",
      "Train Epoch: 12 [14848/60000 (25%)]\tLoss: 0.017831\n",
      "Train Epoch: 12 [30208/60000 (50%)]\tLoss: 0.005821\n",
      "Train Epoch: 12 [45568/60000 (75%)]\tLoss: 0.007898\n",
      "\n",
      "Test set: Average loss: 0.0485, Accuracy: 9875/10000 (99%)\n",
      "\n",
      "Train Epoch: 13 [14848/60000 (25%)]\tLoss: 0.005115\n",
      "Train Epoch: 13 [30208/60000 (50%)]\tLoss: 0.001766\n",
      "Train Epoch: 13 [45568/60000 (75%)]\tLoss: 0.002397\n",
      "\n",
      "Test set: Average loss: 0.0368, Accuracy: 9898/10000 (99%)\n",
      "\n",
      "Train Epoch: 14 [14848/60000 (25%)]\tLoss: 0.001450\n",
      "Train Epoch: 14 [30208/60000 (50%)]\tLoss: 0.000806\n",
      "Train Epoch: 14 [45568/60000 (75%)]\tLoss: 0.000656\n",
      "\n",
      "Test set: Average loss: 0.0379, Accuracy: 9903/10000 (99%)\n",
      "\n",
      "Train Epoch: 15 [14848/60000 (25%)]\tLoss: 0.000788\n",
      "Train Epoch: 15 [30208/60000 (50%)]\tLoss: 0.000625\n",
      "Train Epoch: 15 [45568/60000 (75%)]\tLoss: 0.000713\n",
      "\n",
      "Test set: Average loss: 0.0387, Accuracy: 9910/10000 (99%)\n",
      "\n",
      "Train Epoch: 16 [14848/60000 (25%)]\tLoss: 0.003342\n",
      "Train Epoch: 16 [30208/60000 (50%)]\tLoss: 0.000505\n",
      "Train Epoch: 16 [45568/60000 (75%)]\tLoss: 0.000861\n",
      "\n",
      "Test set: Average loss: 0.0391, Accuracy: 9915/10000 (99%)\n",
      "\n",
      "Train Epoch: 17 [14848/60000 (25%)]\tLoss: 0.000503\n",
      "Train Epoch: 17 [30208/60000 (50%)]\tLoss: 0.000767\n",
      "Train Epoch: 17 [45568/60000 (75%)]\tLoss: 0.000559\n",
      "\n",
      "Test set: Average loss: 0.0399, Accuracy: 9909/10000 (99%)\n",
      "\n",
      "Train Epoch: 18 [14848/60000 (25%)]\tLoss: 0.000462\n",
      "Train Epoch: 18 [30208/60000 (50%)]\tLoss: 0.000442\n",
      "Train Epoch: 18 [45568/60000 (75%)]\tLoss: 0.000241\n",
      "\n",
      "Test set: Average loss: 0.0389, Accuracy: 9914/10000 (99%)\n",
      "\n",
      "Train Epoch: 19 [14848/60000 (25%)]\tLoss: 0.000246\n",
      "Train Epoch: 19 [30208/60000 (50%)]\tLoss: 0.000428\n",
      "Train Epoch: 19 [45568/60000 (75%)]\tLoss: 0.000348\n",
      "\n",
      "Test set: Average loss: 0.0408, Accuracy: 9911/10000 (99%)\n",
      "\n",
      "Train Epoch: 20 [14848/60000 (25%)]\tLoss: 0.000324\n",
      "Train Epoch: 20 [30208/60000 (50%)]\tLoss: 0.000232\n",
      "Train Epoch: 20 [45568/60000 (75%)]\tLoss: 0.000389\n",
      "\n",
      "Test set: Average loss: 0.0402, Accuracy: 9916/10000 (99%)\n",
      "\n",
      "Train Epoch: 1 [14848/60000 (25%)]\tLoss: 0.413567\n",
      "Train Epoch: 1 [30208/60000 (50%)]\tLoss: 0.302881\n",
      "Train Epoch: 1 [45568/60000 (75%)]\tLoss: 0.236143\n",
      "\n",
      "Test set: Average loss: 0.2087, Accuracy: 9398/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [14848/60000 (25%)]\tLoss: 0.206720\n",
      "Train Epoch: 2 [30208/60000 (50%)]\tLoss: 0.156368\n",
      "Train Epoch: 2 [45568/60000 (75%)]\tLoss: 0.144245\n",
      "\n",
      "Test set: Average loss: 0.1165, Accuracy: 9658/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [14848/60000 (25%)]\tLoss: 0.102006\n",
      "Train Epoch: 3 [30208/60000 (50%)]\tLoss: 0.101541\n",
      "Train Epoch: 3 [45568/60000 (75%)]\tLoss: 0.075749\n",
      "\n",
      "Test set: Average loss: 0.0811, Accuracy: 9748/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [14848/60000 (25%)]\tLoss: 0.097765\n",
      "Train Epoch: 4 [30208/60000 (50%)]\tLoss: 0.061561\n",
      "Train Epoch: 4 [45568/60000 (75%)]\tLoss: 0.067049\n",
      "\n",
      "Test set: Average loss: 0.0646, Accuracy: 9798/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [14848/60000 (25%)]\tLoss: 0.079034\n",
      "Train Epoch: 5 [30208/60000 (50%)]\tLoss: 0.033880\n",
      "Train Epoch: 5 [45568/60000 (75%)]\tLoss: 0.050570\n",
      "\n",
      "Test set: Average loss: 0.0506, Accuracy: 9838/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [14848/60000 (25%)]\tLoss: 0.037427\n",
      "Train Epoch: 6 [30208/60000 (50%)]\tLoss: 0.040505\n",
      "Train Epoch: 6 [45568/60000 (75%)]\tLoss: 0.038593\n",
      "\n",
      "Test set: Average loss: 0.0443, Accuracy: 9865/10000 (99%)\n",
      "\n",
      "Train Epoch: 7 [14848/60000 (25%)]\tLoss: 0.028597\n",
      "Train Epoch: 7 [30208/60000 (50%)]\tLoss: 0.026991\n",
      "Train Epoch: 7 [45568/60000 (75%)]\tLoss: 0.049836\n",
      "\n",
      "Test set: Average loss: 0.0423, Accuracy: 9856/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [14848/60000 (25%)]\tLoss: 0.031852\n",
      "Train Epoch: 8 [30208/60000 (50%)]\tLoss: 0.034770\n",
      "Train Epoch: 8 [45568/60000 (75%)]\tLoss: 0.041138\n",
      "\n",
      "Test set: Average loss: 0.0420, Accuracy: 9861/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [14848/60000 (25%)]\tLoss: 0.025100\n",
      "Train Epoch: 9 [30208/60000 (50%)]\tLoss: 0.047872\n",
      "Train Epoch: 9 [45568/60000 (75%)]\tLoss: 0.039142\n",
      "\n",
      "Test set: Average loss: 0.0384, Accuracy: 9875/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [14848/60000 (25%)]\tLoss: 0.043267\n",
      "Train Epoch: 10 [30208/60000 (50%)]\tLoss: 0.015074\n",
      "Train Epoch: 10 [45568/60000 (75%)]\tLoss: 0.023499\n",
      "\n",
      "Test set: Average loss: 0.0400, Accuracy: 9869/10000 (99%)\n",
      "\n",
      "Train Epoch: 11 [14848/60000 (25%)]\tLoss: 0.038535\n",
      "Train Epoch: 11 [30208/60000 (50%)]\tLoss: 0.029369\n",
      "Train Epoch: 11 [45568/60000 (75%)]\tLoss: 0.030641\n",
      "\n",
      "Test set: Average loss: 0.0407, Accuracy: 9870/10000 (99%)\n",
      "\n",
      "Train Epoch: 12 [14848/60000 (25%)]\tLoss: 0.024874\n",
      "Train Epoch: 12 [30208/60000 (50%)]\tLoss: 0.018689\n",
      "Train Epoch: 12 [45568/60000 (75%)]\tLoss: 0.016651\n",
      "\n",
      "Test set: Average loss: 0.0372, Accuracy: 9882/10000 (99%)\n",
      "\n",
      "Train Epoch: 13 [14848/60000 (25%)]\tLoss: 0.016395\n",
      "Train Epoch: 13 [30208/60000 (50%)]\tLoss: 0.040196\n",
      "Train Epoch: 13 [45568/60000 (75%)]\tLoss: 0.008175\n",
      "\n",
      "Test set: Average loss: 0.0410, Accuracy: 9879/10000 (99%)\n",
      "\n",
      "Train Epoch: 14 [14848/60000 (25%)]\tLoss: 0.009044\n",
      "Train Epoch: 14 [30208/60000 (50%)]\tLoss: 0.005971\n",
      "Train Epoch: 14 [45568/60000 (75%)]\tLoss: 0.031824\n",
      "\n",
      "Test set: Average loss: 0.0405, Accuracy: 9874/10000 (99%)\n",
      "\n",
      "Train Epoch: 15 [14848/60000 (25%)]\tLoss: 0.019030\n",
      "Train Epoch: 15 [30208/60000 (50%)]\tLoss: 0.014550\n",
      "Train Epoch: 15 [45568/60000 (75%)]\tLoss: 0.012957\n",
      "\n",
      "Test set: Average loss: 0.0348, Accuracy: 9893/10000 (99%)\n",
      "\n",
      "Train Epoch: 16 [14848/60000 (25%)]\tLoss: 0.012712\n",
      "Train Epoch: 16 [30208/60000 (50%)]\tLoss: 0.017302\n",
      "Train Epoch: 16 [45568/60000 (75%)]\tLoss: 0.010754\n",
      "\n",
      "Test set: Average loss: 0.0469, Accuracy: 9856/10000 (99%)\n",
      "\n",
      "Train Epoch: 17 [14848/60000 (25%)]\tLoss: 0.022325\n",
      "Train Epoch: 17 [30208/60000 (50%)]\tLoss: 0.023383\n",
      "Train Epoch: 17 [45568/60000 (75%)]\tLoss: 0.006673\n",
      "\n",
      "Test set: Average loss: 0.0386, Accuracy: 9887/10000 (99%)\n",
      "\n",
      "Train Epoch: 18 [14848/60000 (25%)]\tLoss: 0.005394\n",
      "Train Epoch: 18 [30208/60000 (50%)]\tLoss: 0.003672\n",
      "Train Epoch: 18 [45568/60000 (75%)]\tLoss: 0.004856\n",
      "\n",
      "Test set: Average loss: 0.0388, Accuracy: 9883/10000 (99%)\n",
      "\n",
      "Train Epoch: 19 [14848/60000 (25%)]\tLoss: 0.005439\n",
      "Train Epoch: 19 [30208/60000 (50%)]\tLoss: 0.010379\n",
      "Train Epoch: 19 [45568/60000 (75%)]\tLoss: 0.022189\n",
      "\n",
      "Test set: Average loss: 0.0375, Accuracy: 9886/10000 (99%)\n",
      "\n",
      "Train Epoch: 20 [14848/60000 (25%)]\tLoss: 0.001403\n",
      "Train Epoch: 20 [30208/60000 (50%)]\tLoss: 0.006398\n",
      "Train Epoch: 20 [45568/60000 (75%)]\tLoss: 0.010968\n",
      "\n",
      "Test set: Average loss: 0.0393, Accuracy: 9893/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with SummaryWriter(comment='struc_diff') as writer:\n",
    "    for epoch in range(1, EPOCHS + 1):#可以看出一个epoch就是数据集上的所有样本都跑完一遍\n",
    "        train(model4, DEVICE, train_loader, optimizer4, epoch)        \n",
    "        test(model4, DEVICE, test_loader)\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train(model2, DEVICE, train_loader, optimizer2, epoch)        \n",
    "        test(model2, DEVICE, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writer=SummaryWriter('D:\\\\Document\\\\python_text\\\\runs'+'demo1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用hook函数提取特征图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_batch,label_batch=next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1(data_batch).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 5, 0, 3, 8, 2, 9, 4, 9, 8, 3, 4, 8, 3, 3, 1, 6, 7, 5, 4, 1, 4, 2, 4,\n",
       "        4, 4, 4, 3, 3, 2, 4, 7, 0, 8, 9, 1, 7, 8, 0, 2, 0, 2, 1, 7, 3, 6, 1, 8,\n",
       "        4, 0, 3, 6, 6, 7, 6, 3, 4, 2, 7, 4, 7, 1, 5, 6, 8, 0, 3, 5, 6, 1, 7, 4,\n",
       "        2, 1, 4, 2, 5, 1, 5, 9, 4, 1, 8, 4, 4, 7, 1, 8, 6, 4, 1, 4, 5, 1, 8, 1,\n",
       "        9, 1, 8, 3, 9, 3, 1, 7, 6, 1, 9, 0, 1, 8, 8, 7, 2, 7, 0, 6, 7, 6, 3, 5,\n",
       "        3, 6, 0, 1, 3, 6, 4, 4, 7, 8, 3, 7, 6, 8, 3, 6, 4, 0, 9, 3, 8, 9, 0, 6,\n",
       "        8, 4, 9, 2, 1, 1, 5, 1, 6, 5, 4, 1, 1, 7, 5, 3, 5, 1, 1, 5, 4, 7, 4, 9,\n",
       "        4, 6, 2, 0, 8, 6, 2, 5, 5, 2, 3, 5, 7, 5, 1, 9, 7, 3, 3, 6, 7, 3, 9, 6,\n",
       "        6, 7, 0, 4, 6, 2, 0, 6, 6, 1, 2, 6, 0, 8, 3, 3, 6, 5, 4, 1, 2, 6, 3, 3,\n",
       "        8, 6, 6, 2, 8, 8, 9, 5, 9, 3, 9, 4, 5, 5, 8, 9, 1, 5, 6, 1, 8, 6, 2, 7,\n",
       "        8, 5, 8, 2, 9, 7, 2, 2, 8, 3, 8, 8, 5, 3, 0, 8, 7, 7, 5, 3, 5, 8, 0, 5,\n",
       "        1, 8, 4, 2, 5, 9, 4, 9, 4, 1, 3, 7, 4, 4, 2, 6, 5, 5, 1, 0, 2, 4, 1, 9,\n",
       "        2, 4, 6, 4, 6, 7, 6, 1, 9, 4, 7, 8, 3, 6, 4, 6, 6, 3, 7, 4, 1, 7, 6, 6,\n",
       "        4, 0, 9, 8, 2, 9, 4, 2, 1, 7, 9, 3, 1, 2, 8, 3, 0, 3, 3, 6, 5, 3, 6, 1,\n",
       "        6, 1, 0, 1, 2, 9, 2, 1, 5, 4, 0, 2, 3, 5, 8, 2, 8, 8, 1, 7, 3, 5, 2, 5,\n",
       "        9, 7, 2, 4, 3, 8, 3, 5, 4, 5, 4, 0, 3, 5, 6, 2, 3, 3, 9, 1, 2, 3, 9, 9,\n",
       "        8, 2, 6, 3, 1, 6, 2, 4, 9, 1, 2, 4, 6, 9, 9, 2, 8, 8, 1, 5, 8, 0, 9, 3,\n",
       "        7, 6, 1, 5, 5, 0, 8, 6, 1, 5, 9, 1, 4, 7, 3, 9, 7, 2, 8, 5, 7, 7, 6, 0,\n",
       "        0, 2, 7, 6, 1, 5, 9, 6, 2, 5, 3, 5, 6, 6, 0, 1, 2, 4, 4, 2, 5, 3, 0, 3,\n",
       "        1, 6, 1, 5, 3, 6, 2, 1, 1, 9, 3, 4, 3, 0, 2, 0, 2, 1, 1, 1, 7, 2, 2, 1,\n",
       "        9, 7, 2, 4, 4, 3, 1, 3, 4, 7, 5, 4, 0, 8, 4, 5, 9, 6, 4, 2, 1, 2, 1, 9,\n",
       "        8, 7, 4, 5, 0, 7, 3, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data_batch[1].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取三个structure的特征图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#提取model1的特征图\n",
    "def forward_hook(module, data_input, data_output):\n",
    "        fmap_block.append(data_output)\n",
    "        input_block.append(data_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmap_block = list()\n",
    "input_block = list()\n",
    "model1.conv2.register_forward_hook(forward_hook)\n",
    "output = model1(data1)\n",
    "a=fmap_block[0].transpose(0,1)\n",
    "with SummaryWriter(comment='struc_diff') as writer:   \n",
    "    fmap_grid = torchvision.utils.make_grid(a, normalize=True, scale_each=True, nrow=10)\n",
    "    writer.add_image('feature_map1 ', fmap_grid, global_step=321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmap_block = list()\n",
    "input_block = list()\n",
    "model2.conv3.register_forward_hook(forward_hook)\n",
    "output = model2(data1)\n",
    "a=fmap_block[0].transpose(0,1)\n",
    "with SummaryWriter(comment='struc_diff') as writer:   \n",
    "    fmap_grid = torchvision.utils.make_grid(a, normalize=True, scale_each=True, nrow=10)\n",
    "    writer.add_image('feature_map2 ', fmap_grid, global_step=321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmap_block = list()\n",
    "input_block = list()\n",
    "model3.conv2.register_forward_hook(forward_hook)\n",
    "output = model3(data1)\n",
    "a=fmap_block[0].transpose(0,1)\n",
    "with SummaryWriter(comment='struc_diff') as writer:   \n",
    "    fmap_grid = torchvision.utils.make_grid(a, normalize=True, scale_each=True, nrow=10)\n",
    "    writer.add_image('feature_map3 ', fmap_grid, global_step=321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'rl2020'",
   "language": "python",
   "name": "rl2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
